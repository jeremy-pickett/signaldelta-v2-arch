SignalDelta Product Architecture — Core Systems Specification
Version 1.0 DRAFT | February 8, 2026
________________


The Four Pillars
SignalDelta is built on four core systems. Each is a first-class citizen — meaning it has its own API domain, its own frontend surface area, its own data model, and its own development roadmap. No pillar is subordinate to another; they are peers that communicate through well-defined interfaces.
┌─────────────────────────────────────────────────────────────────┐
│                     SignalDelta Platform                        │
│                                                                 │
│  ┌──────────────┐  ┌──────────────┐  ┌─────────┐  ┌─────────┐ │
│  │              │  │              │  │         │  │         │ │
│  │  SignalDelta │  │  Conductor   │  │ Cascade │  │  Tempo  │ │
│  │    Score     │  │              │  │         │  │         │ │
│  │              │  │              │  │         │  │         │ │
│  │  "What does  │  │  "What do    │  │ "What   │  │ "What   │ │
│  │  the data    │  │  we do       │  │ chain   │  │ is the  │ │
│  │   say?"      │  │  about it?"  │  │ caused  │  │ normal  │ │
│  │              │  │              │  │  this?"  │  │ for     │ │
│  │              │  │              │  │         │  │  this?"  │ │
│  └──────┬───────┘  └──────┬───────┘  └────┬────┘  └────┬────┘ │
│         │                 │               │             │       │
│         └─────────┬───────┘               │             │       │
│                   │         ┌─────────────┘             │       │
│                   ▼         ▼                           ▼       │
│         ┌─────────────────────────────────────────────────┐     │
│         │              Signal Catalog (80 signals)        │     │
│         │              Collector Layer (140+ collectors)   │     │
│         │              Postgres + Redis                    │     │
│         └─────────────────────────────────────────────────┘     │
└─────────────────────────────────────────────────────────────────┘
________________


Pillar 1: SignalDelta Score
Status: Production. Passed prototype, operating in production with live scoring.
Question it answers: "What does the data say about this security right now?"
What it is: The IALD (Information Asymmetry Leakage Detection) scoring engine. It consumes signals from 80 cataloged signal types across 20 families, applies tier-weighted scoring (S through X), and produces a composite score with a directional verdict (BUY NOW → SELL NOW, 5-tier).
Core components:
* iald_scorer_v3.py — the scoring engine (v3.5 with reddit_sentiment integration)
* confidence_calibrator.py — calibrates signal confidence
* iald_aggregator.py — aggregates across signal families
* Signal catalog — 80 signals across 20 families (S001–S080)
* Collector layer — 140+ collectors feeding the signal pipeline
Data model:
* Input: Signal entries in iald_signals table (generated by collectors)
* Processing: Scorer reads signals, applies weights, computes composite score
* Output: Score + verdict + signal breakdown written to research_progress and rendered as research reports
API surface (v2):
* GET /api/v2/signals/securities/{ticker} — security with current score
* GET /api/v2/signals/research/{ticker} — full IALD research report
* GET /api/v2/signals/radar — real-time signal radar across all securities
* GET /api/v2/signals/heatmap — confidence heatmap visualization data
* POST /api/v2/signals/explain — LLM-powered score explanation
* GET /api/v2/signals/backtest — historical signal backtesting
Frontend surface:
* /research/:ticker — per-security IALD analysis page
* /signals/radar — live signal radar (promoted from Lab)
* /signals/replay — IALD score animation/replay (promoted from Lab)
* /signals/heatmap — confidence heatmap (promoted from Lab)
* /signals/backtest — signal backtesting (promoted from Lab)
* /accuracy — score accuracy accountability
What makes it a first-class citizen:
* Own scoring engine with versioned logic (v3, v3.5)
* Own API domain (/signals/*)
* Own frontend navigation section
* Own accuracy reporting and accountability
* Independent development cadence (signal additions don't require Conductor/Cascade/Tempo changes)
________________


Pillar 2: Conductor
Status: Production. Passed prototype, operating in production with live brokerage integration.
Question it answers: "Given what the score says, what do we do about it?"
What it is: The automated position-taking system. It consumes SignalDelta Scores and executes trading strategies through connected brokerage accounts. Users define strategies (entry/exit rules based on score thresholds), connect brokerages (Alpaca), and Conductor executes positions automatically.
Core components:
* conductor.py (API router) — strategy CRUD, execution management
* services/conductor.ts (frontend service) — brokerage API, strategy API, execution API
* Brokerage integration layer (Alpaca paper + live trading)
* Strategy engine — rule-based position entry/exit based on score thresholds
* Execution tracker — records all trades with attribution to score/strategy
Data model:
* Input: SignalDelta Scores from Pillar 1
* Processing: Strategy rules evaluate scores against thresholds → generate position decisions
* Output: Trade executions logged to conductor_executions, synced with brokerage
API surface (v2):
* GET/POST /api/v2/signals/conductor/brokerages — brokerage connections
* GET/POST /api/v2/signals/conductor/strategies — strategy CRUD
* GET /api/v2/signals/conductor/executions — execution history + stats
* POST /api/v2/signals/conductor/strategies/{id}/apply — apply strategy to securities
Frontend surface:
* /conductor — dashboard with brokerage status, strategy overview, execution feed
* /conductor/strategies — strategy list and management
* /conductor/strategies/new — strategy creation wizard
* /conductor/strategies/:id/apply — apply strategy to securities
* /conductor/executions — execution history with P&L
What makes it a first-class citizen:
* Own brokerage integration layer
* Own strategy engine with independent rule logic
* Own API domain (under /signals/conductor/*)
* Own frontend navigation section (5 routes)
* Own execution tracking and P&L accountability
* Independent development cadence (strategy improvements don't require Score changes)
Relationship to Score: Conductor is a consumer of Score. It reads scores but never writes them. Score has no knowledge of Conductor. This is a one-way dependency: Score → Conductor.
________________


Pillar 3: Cascade
Status: Not yet built. Architecture defined in signal catalog (10 chain types, 20-family cascade mapping). No code exists.
Question it answers: "What chain of events caused this signal pattern? Is this an insider trade, a rug pull, a media leak, or something else?"
What it is: The cascade chain type classifier. It takes the signals that Score consumes and asks a higher-order question: not "how strong are these signals?" (that's Score's job) but "what type of information asymmetry is this?" It classifies signal patterns into one of 10 chain types (CT-01 through CT-10), each representing a distinct institutional pathway through which private information leaks into market behavior.
The 10 chain types:
ID
	Name
	Required Families
	Current Detectability
	CT-01
	Regulatory Cascade
	F02, F03, F09
	Partial
	CT-02
	Insider Trading
	F01, F04, F05
	Partial
	CT-03
	Deal Leakage (M&A)
	F04, F05, F08
	No
	CT-04
	Media Leak
	F09, F04, F01
	Barely
	CT-05
	Crypto Rug Pull
	F14, F15, F16
	Partial (S044 only)
	CT-06
	Earnings Anticipation
	F04, F07, F12
	Partial
	CT-07
	Credit Distress
	F06, F19, F03
	No
	CT-08
	Geopolitical
	F02, F13, F18
	No
	CT-09
	Activist Accumulation
	F05, F08, F04
	No
	CT-10
	Pump-and-Dump
	F16, F14, F10
	No
	Core components (planned):
* Cascade classifier — takes signal sets per security, evaluates against chain type templates
* Chain type templates — define which signal families and temporal orderings characterize each chain type
* Cascade stage model — S0 (Private Cognition) → S1 (Position Building) → S2 (Infrastructure Response) → S3 (Capital Commitment) → S4 (Narrative Formation)
* Cross-family Agreement scoring — signals from different families converging increases cascade confidence
Data model (planned):
* Input: Signal entries from the same signal tables Score reads (shared data layer)
* Processing: Cascade classifier evaluates signal combinations and temporal ordering across families
* Output: Chain type classification with confidence, stage assessment, contributing signals
API surface (v2, reserved):
* GET /api/v2/cascade/{ticker} — cascade analysis for a security
* GET /api/v2/cascade/{ticker}/chain-type — classified chain type with evidence
* GET /api/v2/cascade/{ticker}/stage — current cascade stage assessment
* GET /api/v2/cascade/active — all securities with active cascade detection
Frontend surface (planned):
* /cascade/:ticker — cascade analysis view
* /cascade/active — active cascades dashboard
* Integration into /research/:ticker as a section of the research report
What makes it a first-class citizen:
* Own classification engine (distinct from Score's weighted aggregation)
* Own API domain (/cascade/*)
* Own frontend navigation section
* Own detection logic (temporal ordering, cross-family agreement)
* Answers a different question than Score — "what type?" vs. "how strong?"
Relationship to Score: Cascade reads the same signals as Score but processes them differently. Score asks "how much asymmetry?" — Cascade asks "what kind of asymmetry?" They are parallel consumers of the signal layer, not sequential. Cascade does NOT depend on Score's output; it depends on Score's input (raw signals).
Relationship to Conductor: Cascade output enriches Conductor's decision-making. A Conductor strategy can incorporate chain type: "only enter positions where Cascade classifies CT-02 (Insider Trading) with >60% confidence." This makes Conductor smarter but doesn't make Conductor dependent on Cascade — Conductor works without Cascade, it just works better with it.
________________


Pillar 4: Tempo
Status: Not yet built. Conceptually referenced throughout signal catalog ("Tempo context flag," "Tempo baseline"). No code exists.
Question it answers: "What is normal for this security, in this context, at this time?"
What it is: The contextual baseline engine. It establishes what "normal" looks like for a given security across multiple lenses — historical volatility, sector behavior, seasonal patterns, event calendar proximity, market regime — so that Score and Cascade can measure deviation from normal rather than applying static thresholds.
Why it matters: The same signal (e.g., 2x average options volume) means very different things for NVDA (where 2x happens weekly around earnings) vs. a mid-cap utility stock (where 2x hasn't happened in 18 months). Without Tempo, Score applies the same threshold to both. With Tempo, Score can ask "is this unusual for this security, at this time?"
Core components (planned):
* Multi-lens baseline engine:
   * Historical self (what's normal for THIS security over N days)
   * Sector peer (what's normal for this security's sector)
   * Market regime (bull/bear/sideways adjusts all baselines)
   * Calendar proximity (pre-earnings, pre-lockup, pre-FOMC adjusts baselines)
   * Seasonal patterns (January effect, tax-loss selling, etc.)
* Anomaly scoring — deviation from baseline expressed as z-score or percentile
* Context flags — "this signal fired, but Tempo says it's within normal range for pre-earnings week" (demotes signal) or "this signal fired, and Tempo says this has never happened for this security" (promotes signal)
Data model (planned):
* Input: Historical signal data, price data, calendar events, sector classifications
* Processing: Compute rolling baselines per security per lens, produce deviation scores
* Output: Context-adjusted signal weights that Score and Cascade consume
API surface (v2, reserved):
* GET /api/v2/tempo/{ticker} — current baseline context for a security
* GET /api/v2/tempo/{ticker}/anomalies — signals exceeding Tempo baselines
* GET /api/v2/tempo/{ticker}/regime — current market regime assessment
* GET /api/v2/tempo/calendar — upcoming events affecting baselines
Frontend surface (planned):
* /tempo/:ticker — baseline context view
* Integration into /research/:ticker as context overlay on signals
* Integration into /signals/heatmap as baseline-adjusted coloring
What makes it a first-class citizen:
* Own baseline computation engine (distinct from Score's aggregation and Cascade's classification)
* Own API domain (/tempo/*)
* Own frontend visualization
* Modulates BOTH Score and Cascade — it's the contextual foundation they both stand on
Relationship to Score: Tempo provides context that Score uses to adjust signal weights. Without Tempo, Score uses static thresholds. With Tempo, Score uses contextually adjusted thresholds. Tempo makes Score smarter but Score operates without Tempo (it just uses cruder thresholds).
Relationship to Cascade: Tempo helps Cascade discriminate between "unusual signal pattern = cascade in progress" and "unusual signal pattern = normal for pre-earnings week." Tempo reduces Cascade's false positive rate.
Relationship to Conductor: Tempo indirectly improves Conductor by improving Score and Cascade, which Conductor consumes. Conductor doesn't interact with Tempo directly.
________________


System Interaction Model
                    ┌──────────┐
                    │  Tempo   │
                    │ (context │
                    │ baselines│)
                    └────┬─────┘
                         │ provides context to
                    ┌────┴─────┐
                    ▼          ▼
             ┌──────────┐  ┌─────────┐
             │  Score   │  │ Cascade │
             │ (how     │  │ (what   │
             │  much?)  │  │  type?) │
             └────┬─────┘  └────┬────┘
                  │             │
                  │  both feed  │
                  ▼             ▼
             ┌─────────────────────┐
             │     Conductor      │
             │  (what do we do?)  │
             └─────────────────────┘
Data flow:
1. Collectors gather raw data → write to signal tables
2. Tempo reads historical signal data → computes baselines → writes context adjustments
3. Score reads signals + Tempo context → computes weighted composite score → writes score + verdict
4. Cascade reads signals + Tempo context → classifies chain type → writes classification + stage
5. Conductor reads Score output + Cascade classification → evaluates strategies → executes positions
Independence guarantees:
* Score works without Cascade, Tempo, or Conductor
* Cascade works without Score or Conductor (needs signals, benefits from Tempo)
* Conductor works without Cascade (needs Score, benefits from Cascade)
* Tempo is a pure service — it writes context, others read it. It depends on nothing except historical data.
* Any pillar can be developed, deployed, and tested independently
________________


API v2 Domain Structure (Updated)
/api/v2/
├── signals/                    # Pillar 1: SignalDelta Score
│   ├── securities/             # Security master data + scores
│   ├── research/               # IALD research reports
│   ├── radar/                  # Real-time signal radar
│   ├── replay/                 # Score replay data
│   ├── heatmap/                # Confidence heatmap data
│   ├── backtest/               # Signal backtesting
│   ├── explain/                # LLM score explanation
│   ├── cohorts/                # Cohort analytics
│   ├── deepdelta/              # Deep research analysis
│   └── conductor/              # Pillar 2: Conductor (nested under signals for now)
│       ├── brokerages/
│       ├── strategies/
│       └── executions/
├── cascade/                    # Pillar 3: Cascade (reserved, own domain)
│   ├── {ticker}/
│   ├── active/
│   └── chain-types/
├── tempo/                      # Pillar 4: Tempo (reserved, own domain)
│   ├── {ticker}/
│   ├── anomalies/
│   ├── regime/
│   └── calendar/
├── market/                     # Market data (prices, currencies)
│   ├── prices/
│   └── currencies/
├── users/                      # User management
│   ├── auth/
│   ├── profiles/
│   ├── watchlist/
│   ├── alerts/
│   └── tracking/
└── platform/                   # Platform operations
    ├── subscriptions/
    ├── api-keys/
    ├── admin/
    ├── status/
    └── notifications/
Design decision: Conductor nests under /signals/conductor/ rather than getting its own top-level domain. Reasoning: Conductor is tightly coupled to Score — it consumes scores and acts on them. It doesn't make sense at /conductor/ independently because it has no meaning without Score context. When Cascade and Tempo arrive, Conductor may consume their output too, but the primary dependency is always Score.
If this nesting feels wrong — if Conductor's independence from Score grows as it develops its own strategy logic — it can be promoted to /api/v2/conductor/ as a top-level domain in a future version. The route is reserved.
________________


Build Sequence
Phase
	Pillars Active
	What Ships
	v2.0 (current target)
	Score ✓, Conductor ✓
	Professionalized Score + Conductor, API v2, frontend restructure
	v2.1
	Score, Conductor, Tempo
	Contextual baselines, Score threshold adjustment
	v2.2
	Score, Conductor, Tempo, Cascade
	Chain type classification, cascade detection
	v2.3
	All four + integration
	Conductor consumes Cascade, Tempo-adjusted everything
	Tempo before Cascade because: Tempo makes Score immediately better (fewer false positives on pre-earnings noise, seasonal adjustments). Cascade requires multiple signal families to be active before it can classify chain types — and the family gap analysis shows many families are still blocked/broken. Building Tempo while fixing signal family gaps gives Cascade a stronger foundation when it arrives.
________________


Cross-References
This document is the canonical reference for SignalDelta's four-pillar architecture. The following documents should be read in conjunction:
* API v2 Architecture & Design Specification — detailed API design, auth, LLM gateway, concurrency
* Frontend v2 Design Specification — route structure, component architecture, frontend implementation
* Master Signal Catalog (S001–S080) — signal-level detail feeding all four pillars
* Family Coverage Gap Analysis — which signal families are active, blocked, or empty
* Collector ↔ Signal Gap Reconciliation — where collector data exists but isn't integrated
* IALD Foundation v2 — theoretical framework (H1–H7 hypotheses, S0–S4 stages) underpinning Cascade
________________


DRAFT — February 2026 — SignalDelta • signaldelta.io Classification: CONFIDENTIAL